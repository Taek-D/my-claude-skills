# Kaggle Tabular Playground Series - Time Series Forecasting í•™ìŠµ

> ğŸ’¡ **"ì‹œê³„ì—´ ì˜ˆì¸¡ ê¸°ë²•ì„ Kaggle ê²½ì§„ëŒ€íšŒë¡œ ë§ˆìŠ¤í„°"**
>
> Prophet, XGBoost, LSTM ì•™ìƒë¸”ë¡œ RMSE 0.237 ë‹¬ì„±, 3,200ëª… ì¤‘ ìƒìœ„ 8% (263ë“±), ì‹œê³„ì—´ ë¶„ì„ ì—­ëŸ‰ ì‹¤ë¬´ ì ìš© ì¤€ë¹„ ì™„ë£Œ

---

## ğŸ¯ Goal & Context

**Why I Started This**

ì‹¤ë¬´ì—ì„œ ì¬ê³  ì˜ˆì¸¡, ìˆ˜ìš” ì˜ˆì¸¡ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ë©´ì„œ ì‹œê³„ì—´ ë°ì´í„° íŠ¹ìœ ì˜ íŠ¸ë Œë“œ, ê³„ì ˆì„±, ë…¸ì´ì¦ˆ ì²˜ë¦¬ì— ì–´ë ¤ì›€ì„ ê²ªìŒ. Pandasì˜ ê¸°ë³¸ rolling meanë§Œ ì‚¬ìš©í–ˆê³ , ARIMA, Prophet ê°™ì€ ì „ë¬¸ ì‹œê³„ì—´ ëª¨ë¸ì€ ì´ë¡ ë§Œ ì•Œê³  ì‹¤ì „ ê²½í—˜ ë¶€ì¡±.

Kaggle Tabular Playground Series (2024ë…„ 6ì›”) ëŒ€íšŒê°€ ì‹œê³„ì—´ ì˜ˆì¸¡ ì£¼ì œë¡œ ì§„í–‰ë˜ì–´, ì‹¤ì „ ë°ì´í„°ë¡œ ë‹¤ì–‘í•œ ê¸°ë²•ì„ ì²´ê³„ì ìœ¼ë¡œ í•™ìŠµí•  ê¸°íšŒë¡œ íŒë‹¨.

**Learning Goals**

â€¢ **ê¸°ìˆ  ëª©í‘œ**: 
  - ARIMA, SARIMA, Prophet ëª¨ë¸ êµ¬í˜„ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
  - LSTM/GRU ë”¥ëŸ¬ë‹ ì‹œê³„ì—´ ëª¨ë¸ í•™ìŠµ
  - Feature Engineering for Time Series (Lag, Rolling, Fourier)
  - ì•™ìƒë¸” ê¸°ë²• (Stacking, Blending)

â€¢ **ì‹¤ë¬´ ëª©í‘œ**: 
  - ì¬ê³  ì˜ˆì¸¡ ëª¨ë¸ì— Prophet ì ìš© (ëª©í‘œ: MAPE 12% â†’ 8%)
  - íŠ¸ë Œë“œ/ê³„ì ˆì„± ë¶„í•´ ëŠ¥ë ¥ìœ¼ë¡œ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ

â€¢ **ì„±ê³¼ ëª©í‘œ**: 
  - Kaggle ìƒìœ„ 10% ì§„ì… (Bronze Medal)
  - 5ê°œ ì´ìƒ ì„œë¡œ ë‹¤ë¥¸ ì ‘ê·¼ë²• ì‹¤í—˜
  - ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œê³„ì—´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

---

## ğŸ“Š Learning Journey

**Learning Path**

| Phase | Focus | Duration | Key Milestone |
|-------|-------|----------|---------------|
| Week 1-2 | ì´ë¡  í•™ìŠµ & EDA | 2ì£¼ | Coursera "Time Series" ìˆ˜ë£Œ, ë°ì´í„° ë¶„ì„ ì™„ë£Œ |
| Week 3-4 | Classical Methods | 2ì£¼ | ARIMA/SARIMA êµ¬í˜„, Baseline RMSE 0.312 |
| Week 5-6 | ML & DL Models | 2ì£¼ | XGBoost 0.258, LSTM 0.271 ë‹¬ì„± |
| Week 7-8 | Ensemble & Tuning | 2ì£¼ | 3ëª¨ë¸ Stacking â†’ RMSE 0.237, ìƒìœ„ 8% |
| **Total** | | **8ì£¼** | **263/3,200ë“±, Bronze Medal** |

**Resources Used**

â€¢ **Primary**: 
  - Coursera "Practical Time Series Analysis" (Duke University)
  - Kaggle Learn "Time Series" ì½”ìŠ¤
  - "Forecasting: Principles and Practice" (Hyndman & Athanasopoulos)

â€¢ **Supplementary**: 
  - Prophet ê³µì‹ ë¬¸ì„œ + íŠœí† ë¦¬ì–¼ 10ê°œ
  - LSTM for Time Series ë…¼ë¬¸ 3í¸
  - Kaggle Discussion ë° Notebook 50+ ê°œ ì°¸ê³ 

â€¢ **Total Time**: 120ì‹œê°„ (ì£¼ 15ì‹œê°„ Ã— 8ì£¼)

---

## ğŸ’¡ Key Learnings

### Before & After

**Before Learning**

â€¢ ì‹œê³„ì—´ ì§€ì‹: Pandas rolling mean, ë‹¨ìˆœ ì„ í˜• íšŒê·€ë§Œ ì‚¬ìš©
â€¢ ê³„ì ˆì„± ì²˜ë¦¬: ì›”ë³„ ë”ë¯¸ ë³€ìˆ˜ë¡œ ë¬´ì‹í•˜ê²Œ ì²˜ë¦¬
â€¢ ê²€ì¦ ë°©ë²•: Train/Test Splitë§Œ ì‚¬ìš© (Time Series CV ëª¨ë¦„)
â€¢ ë”¥ëŸ¬ë‹: LSTMì´ ë­”ì§€ëŠ” ì•Œì§€ë§Œ êµ¬í˜„ ê²½í—˜ ì „ë¬´

**After Learning**

â€¢ ì‹œê³„ì—´ ì „ë¬¸ ëª¨ë¸: Prophet, SARIMA, LSTM ëª¨ë‘ êµ¬í˜„ ê°€ëŠ¥
â€¢ Feature Engineering: 15ê°€ì§€ ì‹œê³„ì—´ í”¼ì²˜ (Lag, Rolling, Fourier, Holiday ë“±)
â€¢ ê²€ì¦ ì „ëµ: Time Series Cross-Validation + Walk-Forward ê²€ì¦ ìˆ™ë‹¬
â€¢ ì•™ìƒë¸”: 3ê°œ ëª¨ë¸ Stackingìœ¼ë¡œ ê°œë³„ ëª¨ë¸ ëŒ€ë¹„ **+8% ì„±ëŠ¥ í–¥ìƒ**
â€¢ **ì •ëŸ‰ì  ì„±ê³¼**: Competition RMSE 0.312 (Baseline) â†’ 0.237 (Final) **-24%**

---

### Core Concepts Mastered

**Concept 1: Time Series Feature Engineering**

**What I Learned**

ì‹œê³„ì—´ ë°ì´í„°ëŠ” ê³¼ê±° ê´€ì¸¡ê°’(Lag)ê³¼ í†µê³„ëŸ‰(Rolling)ì´ í•µì‹¬ ì˜ˆì¸¡ ë³€ìˆ˜. Fourier Transformìœ¼ë¡œ ë³µì¡í•œ ê³„ì ˆì„±ë„ ìˆ˜í•™ì ìœ¼ë¡œ ëª¨ë¸ë§ ê°€ëŠ¥.

**How I Applied**

```python
import pandas as pd
import numpy as np

def create_time_features(df, date_col, target_col, lags=[1, 7, 14, 30]):
    """
    ì‹œê³„ì—´ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ íŒŒì´í”„ë¼ì¸
    """
    df = df.copy()
    df[date_col] = pd.to_datetime(df[date_col])
    df = df.sort_values(date_col)
    
    # 1. Lag Features
    for lag in lags:
        df[f'lag_{lag}'] = df[target_col].shift(lag)
    
    # 2. Rolling Statistics (7ì¼, 30ì¼)
    for window in [7, 30]:
        df[f'rolling_mean_{window}'] = df[target_col].rolling(window).mean()
        df[f'rolling_std_{window}'] = df[target_col].rolling(window).std()
        df[f'rolling_min_{window}'] = df[target_col].rolling(window).min()
        df[f'rolling_max_{window}'] = df[target_col].rolling(window).max()
    
    # 3. Time-based Features
    df['day_of_week'] = df[date_col].dt.dayofweek
    df['day_of_month'] = df[date_col].dt.day
    df['month'] = df[date_col].dt.month
    df['quarter'] = df[date_col].dt.quarter
    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
    
    # 4. Fourier Features for Seasonality (ì—°ì£¼ê¸°)
    df['sin_day_of_year'] = np.sin(2 * np.pi * df[date_col].dt.dayofyear / 365.25)
    df['cos_day_of_year'] = np.cos(2 * np.pi * df[date_col].dt.dayofyear / 365.25)
    
    return df

# ì ìš© ê²°ê³¼
df_featured = create_time_features(df, 'date', 'sales', lags=[1, 7, 14, 30])
print(f"Original features: 2, After engineering: {df_featured.shape[1]}")
# Output: Original features: 2, After engineering: 23

# Impact: XGBoost RMSE 0.298 â†’ 0.258 (-13%)
```

**Impact**
â€¢ Feature ìˆ˜: 2ê°œ â†’ 23ê°œ ìƒì„±
â€¢ XGBoost ì„±ëŠ¥: RMSE 0.298 â†’ 0.258 (**-13%**)
â€¢ **Key Insight**: Lag 7ì¼ê³¼ 30ì¼ì´ ê°€ì¥ ì¤‘ìš”í•œ í”¼ì²˜ (Feature Importance ìƒìœ„ 2ê°œ)

---

**Concept 2: Facebook Prophet for Seasonality**

**What I Learned**

Prophetì€ íŠ¸ë Œë“œ, ì—°/ì£¼/ì¼ ê³„ì ˆì„±, íœ´ì¼ íš¨ê³¼ë¥¼ ìë™ìœ¼ë¡œ ëª¨ë¸ë§. ê°€ë²• ëª¨ë¸ (Additive) êµ¬ì¡°ë¡œ ì§ê´€ì  í•´ì„ ê°€ëŠ¥.

**How I Applied**

```python
from prophet import Prophet
import pandas as pd

def train_prophet_model(df, holidays=None):
    """
    Prophet ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡
    """
    # Prophet ìš”êµ¬ í˜•ì‹: ds (date), y (target)
    df_prophet = df[['date', 'sales']].rename(columns={'date': 'ds', 'sales': 'y'})
    
    # ëª¨ë¸ ì´ˆê¸°í™” ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°
    model = Prophet(
        seasonality_mode='multiplicative',  # ê³„ì ˆì„±ì´ íŠ¸ë Œë“œì— ë¹„ë¡€
        yearly_seasonality=True,
        weekly_seasonality=True,
        daily_seasonality=False,
        changepoint_prior_scale=0.05,  # íŠ¸ë Œë“œ ë³€í™” ë¯¼ê°ë„
        holidays=holidays  # ê³µíœ´ì¼ íš¨ê³¼
    )
    
    # ì»¤ìŠ¤í…€ ê³„ì ˆì„± ì¶”ê°€ (ì›”ë³„)
    model.add_seasonality(name='monthly', period=30.5, fourier_order=5)
    
    # í•™ìŠµ
    model.fit(df_prophet)
    
    # ë¯¸ë˜ ì˜ˆì¸¡ (30ì¼)
    future = model.make_future_dataframe(periods=30)
    forecast = model.predict(future)
    
    return model, forecast

# í•œêµ­ ê³µíœ´ì¼ ì •ë³´ ì¶”ê°€
holidays = pd.DataFrame({
    'holiday': 'new_year',
    'ds': pd.to_datetime(['2024-01-01', '2024-02-09', '2024-02-10']),
    'lower_window': 0,
    'upper_window': 1,
})

model, forecast = train_prophet_model(train_df, holidays)

# ì„±ëŠ¥ í‰ê°€
from sklearn.metrics import mean_squared_error
y_true = test_df['sales'].values
y_pred = forecast['yhat'][-len(test_df):].values
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
print(f"Prophet RMSE: {rmse:.3f}")
# Output: Prophet RMSE: 0.264
```

**Impact**
â€¢ ë‹¨ë… ëª¨ë¸ ì„±ëŠ¥: RMSE 0.264 (ARIMA 0.312 ëŒ€ë¹„ **-15%** ê°œì„ )
â€¢ ê³„ì ˆì„± ìë™ ê°ì§€: ì£¼ê°„ íŒ¨í„´ (ì£¼ë§ -18% íŒë§¤ëŸ‰) ë°œê²¬
â€¢ **Key Insight**: ê³µíœ´ì¼ ì •ë³´ ì¶”ê°€ë¡œ RMSE ì¶”ê°€ -3% ê°œì„ 

---

**Concept 3: LSTM Neural Network for Sequences**

**What I Learned**

LSTMì€ ì¥ê¸° ì˜ì¡´ì„±(Long-term Dependencies)ì„ í•™ìŠµ ê°€ëŠ¥í•œ RNN ë³€í˜•. ì‹œê³„ì—´ì˜ ìˆœì„œ ì •ë³´ë¥¼ ì™„ì „íˆ í™œìš©.

**How I Applied**

```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.preprocessing import MinMaxScaler
import numpy as np

def create_sequences(data, seq_length=30):
    """
    LSTM ì…ë ¥ìš© ì‹œí€€ìŠ¤ ìƒì„±
    seq_length: ê³¼ê±° ëª‡ ì¼ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í• ì§€
    """
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i+seq_length])
        y.append(data[i+seq_length])
    return np.array(X), np.array(y)

# ë°ì´í„° ì •ê·œí™” (LSTMì€ scaling í•„ìˆ˜)
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(train_df[['sales']].values)

# ì‹œí€€ìŠ¤ ìƒì„± (ê³¼ê±° 30ì¼ â†’ ë‹¤ìŒ 1ì¼ ì˜ˆì¸¡)
X_train, y_train = create_sequences(scaled_data, seq_length=30)

# LSTM ëª¨ë¸ êµ¬ì¶•
model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(30, 1)),
    Dropout(0.2),
    LSTM(64, return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1)
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# í•™ìŠµ
history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=32,
    validation_split=0.2,
    verbose=0
)

print(f"Best Val Loss: {min(history.history['val_loss']):.4f}")
# Output: Best Val Loss: 0.0023 (scaled)

# ì˜ˆì¸¡ ë° ì—­ì •ê·œí™”
X_test, y_test = create_sequences(scaled_test_data, seq_length=30)
predictions = model.predict(X_test)
predictions = scaler.inverse_transform(predictions)

# ì„±ëŠ¥
rmse = np.sqrt(mean_squared_error(y_test_original, predictions))
print(f"LSTM RMSE: {rmse:.3f}")
# Output: LSTM RMSE: 0.271
```

**Impact**
â€¢ ë‹¨ë… ì„±ëŠ¥: RMSE 0.271 (Prophetë³´ë‹¤ ì•½ê°„ ë‚®ìŒ)
â€¢ ì•™ìƒë¸” íš¨ê³¼: Prophet + XGBoost + LSTM ìŠ¤íƒœí‚¹ ì‹œ **+8%** ì¶”ê°€ ê°œì„ 
â€¢ **Key Insight**: LSTMì€ ê¸‰ê²©í•œ íŠ¸ë Œë“œ ë³€í™” ê°ì§€ì— ê°•í•¨ (ì˜ˆ: í”„ë¡œëª¨ì…˜ ê¸°ê°„)

---

## ğŸ“ˆ Results & Achievements

### Quantitative Outcomes

| Metric | Target | Achieved | Status |
|--------|--------|----------|--------|
| Competition Rank | Top 20% (640ë“±) | **Top 8% (263ë“±)** | âœ… ì´ˆê³¼ë‹¬ì„± |
| RMSE Score | 0.280 ì´í•˜ | **0.237** | âœ… ì´ˆê³¼ë‹¬ì„± |
| ëª¨ë¸ ì‹¤í—˜ ìˆ˜ | 5ê°œ | **8ê°œ** | âœ… ì´ˆê³¼ë‹¬ì„± |
| í•™ìŠµ ì™„ë£Œìœ¨ | 100% | **100%** | âœ… ë‹¬ì„± |
| Notebook ê³µìœ  | 1ê°œ | **3ê°œ** | âœ… ì´ˆê³¼ë‹¬ì„± |

### Qualitative Achievements

â€¢ **Medal**: Bronze Medal (ìƒìœ„ 10% ì´ë‚´ ìˆ˜ìƒ)
â€¢ **Community**: Kaggle Discussion ë‹µë³€ 5ê°œ (Upvotes ì´ 89ê°œ)
â€¢ **Knowledge Sharing**: Medium ë¸”ë¡œê·¸ "ì‹œê³„ì—´ Feature Engineering ì™„ë²½ ê°€ì´ë“œ" (ì¡°íšŒìˆ˜ 2,800+, í´ë© 127ê°œ)
â€¢ **GitHub**: ì‹œê³„ì—´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì˜¤í”ˆì†ŒìŠ¤ ê³µê°œ (Stars 34ê°œ, Forks 8ê°œ)
â€¢ **Recognition**: Kaggle Notebooks Expert ë“±ê¸‰ ë‹¬ì„± (3ê°œ Notebook í•©ì‚° Upvotes 200+)

---

## ğŸ”„ Real-world Application

**ì‹¤ë¬´ ì ìš© ê³„íš**

**ì ìš© 1: ì¬ê³  ì˜ˆì¸¡ ëª¨ë¸ ê°œì„ **

**í˜„ì¬ ìƒí™©**: ë‹¨ìˆœ ì´ë™ í‰ê·  ê¸°ë°˜ ì˜ˆì¸¡, MAPE 12.3%

**ì ìš© ê³„íš**:
â€¢ Prophetìœ¼ë¡œ ê³„ì ˆì„± ëª¨ë¸ë§ (ì—°/ì›”/ì£¼ íŒ¨í„´)
â€¢ í”„ë¡œëª¨ì…˜, ê³µíœ´ì¼ ì •ë³´ë¥¼ ì™¸ë¶€ ë³€ìˆ˜ë¡œ ì¶”ê°€
â€¢ XGBoostë¡œ ë‚ ì”¨, ê²½ìŸì‚¬ ê°€ê²© ë“± ì¶”ê°€ í”¼ì²˜ ë°˜ì˜

**ê¸°ëŒ€ íš¨ê³¼**:
â€¢ MAPE 12.3% â†’ 8.5% ëª©í‘œ (**-31% ê°œì„ **)
â€¢ ì¬ê³  ë¶€ì¡± ë°œìƒ -40% (í˜„ì¬ ì›” 15ê±´ â†’ 9ê±´)
â€¢ ê³¼ì‰ ì¬ê³  ë¹„ìš© ì›” $8K ì ˆê°

**Timeline**: 
â€¢ Week 1-2: ë°ì´í„° ìˆ˜ì§‘ ë° ì •ì œ
â€¢ Week 3-4: Prophet + XGBoost ëª¨ë¸ ê°œë°œ
â€¢ Week 5-6: A/B í…ŒìŠ¤íŠ¸ (ì‹ ê·œ ëª¨ë¸ vs ê¸°ì¡´ ëª¨ë¸)
â€¢ Week 7+: ì ì§„ì  ë°°í¬ ë° ëª¨ë‹ˆí„°ë§

---

**ì ìš© 2: ìˆ˜ìš” ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•**

**í˜„ì¬ ìƒí™©**: ì—‘ì…€ ê¸°ë°˜ ìˆ˜ë™ ì˜ˆì¸¡, ì—…ë°ì´íŠ¸ ì£¼ 1íšŒ

**ì ìš© ê³„íš**:
â€¢ Streamlitìœ¼ë¡œ ì‹¤ì‹œê°„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ ê°œë°œ
â€¢ Prophet ëª¨ë¸ APIí™” (FastAPI)
â€¢ ì¼ì¼ ìë™ ì¬í•™ìŠµ íŒŒì´í”„ë¼ì¸ (Airflow)

**ê¸°ëŒ€ íš¨ê³¼**:
â€¢ ì˜ˆì¸¡ ì—…ë°ì´íŠ¸ ì£¼ê¸°: ì£¼ 1íšŒ â†’ ë§¤ì¼ (**7ë°° í–¥ìƒ**)
â€¢ ë‹´ë‹¹ì ì‘ì—… ì‹œê°„: ì£¼ 4ì‹œê°„ â†’ 30ë¶„ (**-88%**)
â€¢ ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒìœ¼ë¡œ ë§¤ì¶œ ê¸°íšŒ ì†ì‹¤ -15%

**Timeline**:
â€¢ Week 1-2: Streamlit ëŒ€ì‹œë³´ë“œ í”„ë¡œí† íƒ€ì…
â€¢ Week 3: FastAPI ëª¨ë¸ ì„œë¹™ ê°œë°œ
â€¢ Week 4: Airflow íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
â€¢ Week 5+: ìš´ì˜ íŒ€ êµìœ¡ ë° í”¼ë“œë°± ìˆ˜ì§‘

---

**Immediate Next Steps**

âœ… **Week 1**: ì¬ê³  ë°ì´í„° 6ê°œì›”ì¹˜ ìˆ˜ì§‘ ë° EDA (ì™„ë£Œ)
â³ **Week 2**: Prophet Baseline ëª¨ë¸ êµ¬ì¶• (ì§„í–‰ì¤‘)
ğŸ“… **Week 3-4**: XGBoost í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë° ì•™ìƒë¸”
ğŸ“… **Month 2**: A/B í…ŒìŠ¤íŠ¸ ì§„í–‰ ë° ê²°ê³¼ ë¶„ì„
ğŸ“… **Month 3**: í”„ë¡œë•ì…˜ ë°°í¬ ë° ëª¨ë‹ˆí„°ë§ ì²´ê³„ êµ¬ì¶•

---

## ğŸ§  Key Takeaways

**Technical Insights**

â€¢ **Prophetì˜ ê°•ì **: íŠ¸ë Œë“œ + ê³„ì ˆì„± ìë™ ê°ì§€, í•´ì„ ê°€ëŠ¥ì„± ë†’ìŒ â†’ ë¹„ì¦ˆë‹ˆìŠ¤ íŒ€ ì„¤ë“ ìš©ì´
â€¢ **XGBoostì˜ ê°•ì **: ì™¸ë¶€ ë³€ìˆ˜ (ë‚ ì”¨, í”„ë¡œëª¨ì…˜) í†µí•© ìš©ì´, Feature Importanceë¡œ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
â€¢ **LSTMì˜ ê°•ì **: ê¸‰ê²©í•œ íŒ¨í„´ ë³€í™” ê°ì§€ (ì˜ˆ: ë°”ì´ëŸ´ ì´ë²¤íŠ¸) â†’ ì•™ìƒë¸” ì‹œ ë³´ì™„ íš¨ê³¼

**Learning Process Insights**

â€¢ **íš¨ê³¼ì ì´ì—ˆë˜ ë°©ë²•**: 
  - "ì´ë¡  1ì¼ â†’ ì‹¤ìŠµ 2ì¼" ë¹„ìœ¨ë¡œ ë¹ ë¥¸ í”¼ë“œë°± ë£¨í”„
  - Kaggle Discussionì—ì„œ ë‹¤ë¥¸ ì°¸ê°€ìì˜ ì ‘ê·¼ë²• ë§¤ì¼ 30ë¶„ í•™ìŠµ
  - ì‹¤í—˜ ë¡œê·¸ë¥¼ Notionì— ì •ë¦¬ (8ì£¼ê°„ ì´ 47ê°œ ì‹¤í—˜ ê¸°ë¡)

â€¢ **ì–´ë ¤ì› ë˜ ë¶€ë¶„**: 
  - LSTM í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (Layer ìˆ˜, Units, Dropout) â†’ Grid Searchë¡œ í•´ê²°
  - Time Series CV êµ¬í˜„ (ë¯¸ë˜ ë°ì´í„° ëˆ„ì¶œ ë°©ì§€) â†’ Sklearnì˜ TimeSeriesSplit í™œìš©

â€¢ **ë‹¤ìŒ í•™ìŠµ ê°œì„ ì **: 
  - ì´ë¡  í•™ìŠµ ì‹œê°„ ì¤„ì´ê³  ì‹¤ì „ ë¬¸ì œë¶€í„° ì‹œì‘ (Project-based Learning)
  - ë§¤ì£¼ 1ê°œì”© ë¸”ë¡œê·¸ í¬ìŠ¤íŒ…ìœ¼ë¡œ í•™ìŠµ ë‚´ìš© ì •ë¦¬ â†’ Retention í–¥ìƒ

**Career Growth**

â€¢ **ì—­ëŸ‰ í–¥ìƒ**: Data Analyst â†’ "ì‹œê³„ì—´ ì „ë¬¸" ì—­ëŸ‰ ë³´ìœ  (LinkedIn í”„ë¡œí•„ ì—…ë°ì´íŠ¸)
â€¢ **í˜‘ì—… ê°œì„ **: ì¬ê³  íŒ€ê³¼ "ì˜ˆì¸¡ ëª¨ë¸" ê³µí†µ ì–¸ì–´ë¡œ ì†Œí†µ ê°€ëŠ¥
â€¢ **Next Step**: GNN (Graph Neural Network) í•™ìŠµ â†’ ë„¤íŠ¸ì›Œí¬ íš¨ê³¼ ëª¨ë¸ë§

---

## ğŸ¤ Community Impact

**í•™ìŠµ ë‚´ìš©ì„ ì»¤ë®¤ë‹ˆí‹°ì— í™˜ì›**

**Kaggle Contribution**
â€¢ **Discussion ë‹µë³€**: 5ê°œ í¬ìŠ¤íŒ… (Upvotes ì´ 89ê°œ)
  - "ì‹œê³„ì—´ CV ì‹¤ìˆ˜ í”¼í•˜ê¸°" (34 Upvotes)
  - "Prophet vs ARIMA ì„ íƒ ê°€ì´ë“œ" (28 Upvotes)
  - "LSTM ì˜¤ë²„í”¼íŒ… ë°©ì§€ íŒ" (27 Upvotes)

â€¢ **Notebook ê³µìœ **: 3ê°œ Public Notebook
  - "Time Series Feature Engineering Toolkit" (Upvotes 67, Forks 23)
  - "Prophet + XGBoost Ensemble" (Upvotes 52, Forks 18)
  - "LSTM for Tabular Time Series" (Upvotes 41, Forks 12)

**Medium ë¸”ë¡œê·¸**
â€¢ "ì‹œê³„ì—´ Feature Engineering ì™„ë²½ ê°€ì´ë“œ" (ì¡°íšŒìˆ˜ 2,800+, í´ë© 127ê°œ)
â€¢ "Kaggleì—ì„œ ë°°ìš´ ì‹œê³„ì—´ ì˜ˆì¸¡ ì‹¤ì „ íŒ 10ê°€ì§€" (ì¡°íšŒìˆ˜ 1,600+, í´ë© 83ê°œ)

**GitHub ì˜¤í”ˆì†ŒìŠ¤**
â€¢ `timeseries-toolkit` ë¼ì´ë¸ŒëŸ¬ë¦¬ ê³µê°œ (Stars 34, Forks 8)
  - Feature Engineering ìë™í™”
  - Prophet/ARIMA/LSTM í†µí•© ì¸í„°í˜ì´ìŠ¤
  - Time Series CV í—¬í¼ í•¨ìˆ˜

**ì‚¬ë‚´ ML ìŠ¤í„°ë””**
â€¢ "ì‹œê³„ì—´ ë¶„ì„ ì…ë¬¸" ì„¸ì…˜ ë¦¬ë“œ (ì°¸ì—¬ì 12ëª…, ë§Œì¡±ë„ 4.7/5.0)
â€¢ í•™ìŠµ ìë£Œ ë° ì½”ë“œ ì˜ˆì œ ê³µìœ  (Confluence í˜ì´ì§€ ì¡°íšŒìˆ˜ 180+)

---

## ğŸ”— Links

[Kaggle Profile](https://www.kaggle.com/username) | [Competition Page](ë§í¬) | [GitHub Repo](https://github.com/username/timeseries-toolkit) | [Medium Blog](ë§í¬) | [Best Notebook](ë§í¬)
