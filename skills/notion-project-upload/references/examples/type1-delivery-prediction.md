# Delivery Time Prediction System

> ğŸ’¡ **"XGBoost ì˜ˆì¸¡ ëª¨ë¸ë¡œ ë°°ë‹¬ ì‹œê°„ ì˜¤ì°¨ -56%, í™˜ë¶ˆ ë¹„ìš© -73% ë‹¬ì„±"**
>
> ë¶€ì •í™•í•œ ë°°ë‹¬ ì‹œê°„ ì•ˆë‚´ ë¬¸ì œë¥¼ ML ëª¨ë¸ë¡œ í•´ê²°í•˜ì—¬ ê³ ê° ë§Œì¡±ë„ ê°œì„  ë° ì¬ì£¼ë¬¸ë¥  ì¦ê°€

---

## ğŸ¯ Performance Overview

**30ì´ˆ ìŠ¤ìº”ìš© - í•µì‹¬ ì„±ê³¼**

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Prediction MAE | 23ë¶„ | 10ë¶„ | **-56%** |
| CSAT Score | 3.8/5 | 4.3/5 | **+0.5ì ** |
| í™˜ë¶ˆ ë¹„ìš© | ì›” â‚©1.5M | ì›” â‚©400K | **-73%** |
| ì¬ì£¼ë¬¸ë¥  (30ì¼) | 61% | 66% | **+5%p** |

**Impact Summary**: XGBoost ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶• â†’ MAE 23ë¶„â†’10ë¶„ (-56%) â†’ CSAT +0.5ì , í™˜ë¶ˆ ë¹„ìš© -73% â†’ ì›” â‚©1.1M ì ˆê°

---

## ğŸ“Š Solution Process

### 1ï¸âƒ£ Problem Discovery

**Business Pain Point**

â€¢ ë¶€ì •í™•í•œ ë°°ë‹¬ ì‹œê°„ ì•ˆë‚´ë¡œ ê³ ê° ë¶ˆë§Œ ì¦ê°€
â€¢ CSAT 3.8/5 (ì—…ê³„ í‰ê·  4.2/5 ëŒ€ë¹„ -0.4ì )
â€¢ ë°°ë‹¬ ì§€ì—°ìœ¼ë¡œ ì¸í•œ ì›” í™˜ë¶ˆ ë¹„ìš© â‚©1.5M
â€¢ ì¬ì£¼ë¬¸ë¥  ì €í•˜ (-12%)ë¡œ ë§¤ì¶œ ì†ì‹¤

**Root Cause Analysis**

â€¢ ê¸°ì¡´ ì‹œìŠ¤í…œ: ë‹¨ìˆœ ê±°ë¦¬ ê¸°ë°˜ í‰ê·  ê³„ì‚°
â€¢ êµí†µ í˜¼ì¡ë„, ë‚ ì”¨, í”¼í¬ ì‹œê°„ëŒ€ ë¯¸ë°˜ì˜
â€¢ ëŸ¬ì‹œì•„ì›Œ(12-14ì‹œ, 18-20ì‹œ) ì˜¤ì°¨ ìµœëŒ€ +45ë¶„
â€¢ ìŒì‹ì ë³„ ì¡°ë¦¬ ì‹œê°„ í¸ì°¨ ë¯¸ê³ ë ¤ (í•œì‹ 18ë¶„ vs ì¤‘ì‹ 24ë¶„)

---

### 2ï¸âƒ£ Solution Design

**Approach & Strategy**

ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ë°°ë‹¬ ì‹œê°„ ì˜ˆì¸¡ ëª¨ë¸ë¡œ ì •í™•ë„ í–¥ìƒ

**Solution Options Considered**

| Option | Pros | Cons | Decision |
|--------|------|------|----------|
| Linear Regression | ê°„ë‹¨, ë¹ ë¦„ | MAE 21ë¶„, ì •í™•ë„ ë‚®ìŒ | âŒ ì„ íƒ ì•ˆ í•¨ |
| Random Forest | MAE 12ë¶„ | ì¶”ë¡  ì†ë„ ëŠë¦¼ (45ms) | âŒ ì„ íƒ ì•ˆ í•¨ |
| XGBoost | MAE 10ë¶„, ë¹ ë¦„ (12ms) | í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš” | âœ… **ì„ íƒ** |

**Feature Engineering** (14 features)

â€¢ Geographic (5): ì§ì„  ê±°ë¦¬, ê²½ë¡œ ê±°ë¦¬, êµí†µ í˜¼ì¡ë„, ì‹ í˜¸ë“± ê°œìˆ˜, ì§€ì—­êµ¬
â€¢ Temporal (4): ì‹œê°„ëŒ€, ìš”ì¼, ê³µíœ´ì¼, ì£¼ë¬¸ëŸ‰
â€¢ Contextual (5): ë‚ ì”¨, ì˜¨ë„, ê°•ìˆ˜ëŸ‰, ìŒì‹ì  ì¡°ë¦¬ ì‹œê°„, ë°°ë‹¬ ê¸°ì‚¬ ê²½ë ¥

**A/B Test Design**

â€¢ **Control**: ê¸°ì¡´ í‰ê·  ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ (N=15,000)
â€¢ **Treatment**: XGBoost ì˜ˆì¸¡ ëª¨ë¸ (N=15,000)
â€¢ **Duration**: 2ì£¼
â€¢ **Primary Metric**: MAE (Mean Absolute Error)
â€¢ **Secondary Metrics**: CSAT, ì¬ì£¼ë¬¸ë¥ , í™˜ë¶ˆ ìš”ì²­ë¥ 
â€¢ **Statistical Power**: 95% confidence, Î±=0.05

---

### 3ï¸âƒ£ Implementation

**Tech Stack**

â€¢ **Model**: Python 3.11, XGBoost 2.0, scikit-learn 1.3
â€¢ **Serving**: FastAPI 0.104, Redis 7.0 (model caching)
â€¢ **Infrastructure**: AWS Lambda (prediction API), S3 (model storage)
â€¢ **Monitoring**: Datadog APM, Mixpanel (CSAT tracking)

**System Architecture**

```mermaid
graph LR
    A[Order Request] --> B[FastAPI Gateway]
    B --> C[Redis Cache]
    C -->|Cache Hit| D[Cached Prediction]
    C -->|Cache Miss| E[XGBoost Model]
    E --> F[Feature Engineering]
    F --> G[Prediction 12ms]
    G --> H[Update Cache]
    G --> I[Response]
```

**Core Implementation**

```python
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit

# Feature engineering with 14 features
def create_features(df):
    df['adjusted_distance'] = df['distance'] * df['traffic_factor']
    df['peak_hour'] = df['hour'].isin([12, 13, 18, 19, 20])
    df['weather_penalty'] = df['precipitation'] * 0.3
    # ... 11 more features
    return df

# Time-series cross-validation (6 folds)
tscv = TimeSeriesSplit(n_splits=6)
for train_idx, val_idx in tscv.split(features):
    model = xgb.XGBRegressor(
        max_depth=6,
        learning_rate=0.1,
        n_estimators=200,
        objective='reg:squarederror'
    )
    model.fit(X_train, y_train)
    
# FastAPI prediction endpoint
@app.post("/predict")
async def predict(request: PredictRequest):
    features = extract_features(request)
    prediction = model.predict(features)
    return {"estimated_time": prediction[0]}
```

**Implementation Highlights**

â€¢ Real-time prediction API: í‰ê·  ì‘ë‹µ ì‹œê°„ 12ms
â€¢ Redis caching: ë™ì¼ ê²½ë¡œ ìš”ì²­ ì¦‰ì‹œ ì‘ë‹µ
â€¢ Fallback mechanism: ëª¨ë¸ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìë™ ì „í™˜
â€¢ Feature importance: êµí†µ í˜¼ì¡ë„ (32%), ê±°ë¦¬ (28%), ì‹œê°„ëŒ€ (21%)

---

### 4ï¸âƒ£ Validation & Testing

**Offline Evaluation**

| Metric | Baseline | XGBoost | Improvement |
|--------|----------|---------|-------------|
| MAE | 23ë¶„ | 10ë¶„ | **-56%** |
| P95 | 37ë¶„ | 18ë¶„ | **-51%** |
| RMSE | 28ë¶„ | 13ë¶„ | **-54%** |

**A/B Test Results** (2ì£¼, N=30,000)

â€¢ Treatment MAE: **10.4ë¶„** vs Control: 22.8ë¶„ (-54%, **p<0.001**)
â€¢ CSAT: **4.3/5** vs 3.8/5 (+0.5ì , **p<0.001**)
â€¢ ì¬ì£¼ë¬¸ë¥ : **66%** vs 61% (+5%p, **p=0.003**)
â€¢ í™˜ë¶ˆ ìš”ì²­: **-62%** reduction

**Error Analysis**

â€¢ ë‚ ì”¨ ê·¹ë‹¨ê°’ (í­ìš°/í­ì„¤): ì˜¤ì°¨ +8ë¶„ â†’ ë³„ë„ ë‚ ì”¨ ëª¨ë¸ í•„ìš”
â€¢ ì‹ ê·œ ì§€ì—­: ë°ì´í„° ë¶€ì¡± â†’ Fallback to baseline
â€¢ í”¼í¬ ì‹œê°„ (19-20ì‹œ): ì˜¤ì°¨ Â±6ë¶„ â†’ ê°€ì¥ ì •í™•í•œ êµ¬ê°„

---

### 5ï¸âƒ£ Deployment & Rollout

**Rollout Strategy**

```mermaid
gantt
    title Deployment Timeline
    dateFormat  YYYY-MM-DD
    
    section Testing
    A/B Test           :done, 2024-01-01, 14d
    
    section Rollout
    Canary 5%         :done, 2024-01-15, 2d
    Expand to 25%     :done, 2024-01-17, 3d
    Expand to 50%     :done, 2024-01-20, 3d
    Full Rollout 100% :done, 2024-01-23, 1d
```

â€¢ **Phase 1 (Canary 5%)**: 2ì¼, MAE ëª¨ë‹ˆí„°ë§, ì—ëŸ¬ìœ¨ <1% í™•ì¸
â€¢ **Phase 2 (25%)**: 3ì¼, CSAT ê°œì„  í™•ì¸ (+12 points)
â€¢ **Phase 3 (50%)**: 3ì¼, ì¬ì£¼ë¬¸ë¥  ì¦ê°€ í™•ì¸ (+5%p)
â€¢ **Phase 4 (100%)**: 1ì¼, ì „ì²´ ë¡¤ì•„ì›ƒ

**Monitoring & Alerting**

â€¢ **Datadog**: Prediction latency, error rate, model drift
â€¢ **Mixpanel**: CSAT tracking, ì¬ì£¼ë¬¸ë¥ , í™˜ë¶ˆ ìš”ì²­ë¥ 
â€¢ **Alert**: MAE >15ë¶„ ì‹œ Slack #eng-ml ì•Œë¦¼
â€¢ **Dashboard**: Real-time prediction accuracy, cache hit rate (87%)

**Production Infrastructure**

â€¢ AWS Lambda: Auto-scaling (100 â†’ 500 concurrent)
â€¢ S3: Model versioning, rollback ì§€ì›
â€¢ Redis: Model caching, TTL 90ì´ˆ
â€¢ CI/CD: GitHub Actions â†’ Lambda deployment

---

### 6ï¸âƒ£ Impact Measurement

**Business Impact** (ìš´ì˜ 3ê°œì›” í›„)

| í•­ëª© | Before | After | Impact |
|------|--------|-------|--------|
| CSAT | 3.8/5 | 4.3/5 | **+0.5ì ** |
| ì¬ì£¼ë¬¸ìœ¨ | 61% | 66% | **+5%p** |
| í™˜ë¶ˆ ë¹„ìš© | ì›” â‚©1.5M | ì›” â‚©400K | **-73%** |
| CS ë¶ˆë§Œ ì ‘ìˆ˜ | ì›” 1,200ê±´ | 480ê±´ | **-60%** |

**ë¹„ìš© íš¨ê³¼ ê³„ì‚°**

```
í™˜ë¶ˆ ë¹„ìš© ì ˆê°: ì›” â‚©1.1M (â‚©1.5M â†’ â‚©400K)
CS ëŒ€ì‘ ì‹œê°„ ì ˆê°: ì›” ì•½ 96ì‹œê°„ (720ê±´ Ã— 8ë¶„)
ì—°ê°„ ì ˆê° ì¶”ì •: ìµœì†Œ â‚©13.2M (í™˜ë¶ˆ) + CS ì¸ê±´ë¹„ ì ˆê°
```

**Long-term Impact**

â€¢ 3ê°œì›”: CSAT 4.3 ìœ ì§€, ì¬ì£¼ë¬¸ë¥  66% ì•ˆì •í™”
â€¢ 6ê°œì›”: ë‚ ì”¨ë³„ ì „ìš© ëª¨ë¸, ìŒì‹ì ë³„ ëª¨ë¸ ì¶”ê°€ ê°œì„  ê¸°íšŒ ë°œê²¬
â€¢ 12ê°œì›” ê³„íš: ë‹¤ë¥¸ ë„ì‹œ í™•ì¥ ê²€í† 

---

## ğŸ’¡ Key Takeaways

**"Feature Engineering > Model Selection"**

êµí†µ í˜¼ì¡ë„ ë°˜ì˜í•œ "adjusted_distance" í”¼ì²˜ê°€ MAE 2.3ë¶„ ê°œì„ . Raw dataë³´ë‹¤ ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ í”¼ì²˜ê°€ ì¤‘ìš”.

**ì•„ì‰¬ìš´ ì  & ê°œì„  ë°©í–¥**

â€¢ **í•œê³„ì **: ë‚ ì”¨ ê·¹ë‹¨ê°’ (í­ìš°/í­ì„¤) ì‹œ ì˜¤ì°¨ +8ë¶„, ì‹ ê·œ ì§€ì—­ ë°ì´í„° ë¶€ì¡±
â€¢ **Next Step**: ë‚ ì”¨ë³„ ì „ìš© ëª¨ë¸, ìŒì‹ì ë³„ ì¡°ë¦¬ ì‹œê°„ ì˜ˆì¸¡ ëª¨ë¸ ì¶”ê°€

---

## ğŸ¤ Collaboration & Impact

**My Role**: XGBoost ëª¨ë¸ ê°œë°œ, Feature Engineering, A/B í…ŒìŠ¤íŠ¸ ì„¤ê³„ ë° í†µê³„ ë¶„ì„

**Cross-functional Collaboration**

**ë°°ë‹¬ ìš´ì˜íŒ€**
â€¢ ì£¼ 1íšŒ ì„±ê³¼ ë¦¬ë·° ë¯¸íŒ… â€” ì˜ˆì¸¡ ì •í™•ë„ ëª¨ë‹ˆí„°ë§ ë° ì´ìŠˆ íŠ¸ë˜í‚¹
â€¢ ë°°ë‹¬ ê¸°ì‚¬ ì¸í„°ë·°ë¡œ ì‹¤ì œ ë³€ìˆ˜ íŒŒì•… (ì‹ í˜¸ë“±, ì£¼ì°¨, ì—˜ë¦¬ë² ì´í„° ë“±)
â€¢ í”¼ë“œë°±: "êµí†µ í˜¼ì¡ë„" í”¼ì²˜ ì¶”ê°€ ì œì•ˆ â†’ MAE 1.8ë¶„ ê°œì„ 

**CSíŒ€**
â€¢ ë¶ˆë§Œ ì ‘ìˆ˜ ë‚´ì—­ ë¶„ì„ìœ¼ë¡œ ì£¼ìš” pain point íŒŒì•…
â€¢ "ì˜ˆì¸¡ ì‹ ë¢° êµ¬ê°„" í‘œì‹œ ì œì•ˆ ìˆ˜ìš© â†’ ë°°ë‹¬ ì‹œê°„ ê´€ë ¨ ë¶ˆë§Œ **60% ê°ì†Œ**

**Stakeholder Impact**

â€¢ **ê³ ê°**: ì •í™•í•œ ë„ì°© ì‹œê°„ ì•ˆë‚´ë¡œ CSAT **+0.5ì **
â€¢ **ìš´ì˜**: í™˜ë¶ˆ ë¹„ìš© ì›” **â‚©1.1M ì ˆê°** (-73%)
â€¢ **ë¹„ì¦ˆë‹ˆìŠ¤**: ì¬ì£¼ë¬¸ë¥  **+5%p** ìƒìŠ¹

---

## ğŸ”§ Technical Approach

### Model Architecture

```mermaid
graph LR
    A[Raw Data] --> B[Feature Engineering]
    B --> C[XGBoost Model]
    C --> D[Prediction + CI]
    D --> E[API Response]
    
    B --> F[19 Features]
    F --> F1[ì£¼ë¬¸ ì •ë³´ 5ê°œ]
    F --> F2[ê±°ë¦¬/ê²½ë¡œ 4ê°œ]
    F --> F3[ìŒì‹ì  3ê°œ]
    F --> F4[ë°°ë‹¬ ê¸°ì‚¬ 4ê°œ]
    F --> F5[í™˜ê²½ 3ê°œ]
    
    C --> G[Hyperparameters]
    G --> G1[max_depth: 7]
    G --> G2[learning_rate: 0.05]
    G --> G3[n_estimators: 300]
```

### Key Implementation

**Feature Engineering - Traffic Congestion Score**

```python
def calculate_traffic_congestion(row):
    """êµí†µ í˜¼ì¡ë„ ê³„ì‚° - ì‹œê°„ëŒ€, ìš”ì¼, ë‚ ì”¨ ë°˜ì˜ (0.5~2.5)"""
    base = 1.0
    hour = row['order_hour']
    
    # ì‹œê°„ëŒ€ ê°€ì¤‘ì¹˜ (ëŸ¬ì‹œì•„ì›Œ 1.8x, ì‹¬ì•¼ 0.6x)
    if 12 <= hour <= 14 or 18 <= hour <= 20:
        base *= 1.8
    elif 22 <= hour or hour <= 6:
        base *= 0.6
    
    # ìš”ì¼/ë‚ ì”¨ ê°€ì¤‘ì¹˜
    if row['is_weekend']: base *= 1.2
    if row['weather'] == 'rain': base *= 1.4
    elif row['weather'] == 'snow': base *= 1.8
    
    return np.clip(base, 0.5, 2.5)

# ì ìš© â†’ MAE 9.1ë¶„ â†’ 6.8ë¶„ (-25%)
df['traffic_congestion'] = df.apply(calculate_traffic_congestion, axis=1)
```

**XGBoost Model Training**

```python
from xgboost import XGBRegressor
from sklearn.model_selection import cross_val_score

# XGBoost ëª¨ë¸ ì •ì˜
model = XGBRegressor(
    max_depth=7, learning_rate=0.05,
    n_estimators=300, subsample=0.8,
    colsample_bytree=0.8, random_state=42
)

# 5-Fold Cross-Validation
cv_scores = cross_val_score(model, X_train, y_train, cv=5,
                            scoring='neg_mean_absolute_error')
print(f"CV MAE: {-cv_scores.mean():.2f} Â± {cv_scores.std():.2f}ë¶„")
# Output: CV MAE: 6.95 Â± 0.31ë¶„

model.fit(X_train, y_train)
# Test MAE: 6.82ë¶„
```

---

## ğŸš€ Deployment & Usage

**Production System**

```mermaid
graph TB
    A[ì£¼ë¬¸ ë°œìƒ] --> B[Feature ìˆ˜ì§‘]
    B --> C{Cache Hit?}
    C -->|Yes| D[ìºì‹œëœ ì˜ˆì¸¡ê°’]
    C -->|No| E[Model API í˜¸ì¶œ]
    E --> F[ì˜ˆì¸¡ + ì‹ ë¢°êµ¬ê°„]
    F --> G[Cache ì €ì¥]
    D --> H[ê³ ê° ì•± í‘œì‹œ]
    G --> H
    
    I[Daily Retrain] --> J[ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§]
    J --> K{ì„±ëŠ¥ ì €í•˜?}
    K -->|Yes| L[Alert + ì¬í•™ìŠµ]
    K -->|No| I
```

**Deployment Timeline**

| Phase | Duration | Activity | Result |
|-------|----------|----------|--------|
| ê°œë°œ | 6ì£¼ | ë°ì´í„° ìˆ˜ì§‘, ëª¨ë¸ í•™ìŠµ, API ê°œë°œ | MAE 6.8ë¶„ ë‹¬ì„± |
| A/B í…ŒìŠ¤íŠ¸ | 2ì£¼ | 10% íŠ¸ë˜í”½ í…ŒìŠ¤íŠ¸ | ì´ìŠˆ ì—†ìŒ í™•ì¸ |
| ì ì§„ì  ë°°í¬ | 2ì£¼ | 10% â†’ 50% â†’ 100% | ì•ˆì •ì„± í™•ì¸ |
| ëª¨ë‹ˆí„°ë§ | ì§„í–‰ì¤‘ | ì¼ì¼ ì„±ëŠ¥ ì²´í¬, ì£¼ê°„ ë¦¬í¬íŠ¸ | CSAT ì§€ì† ìƒìŠ¹ |

**Current Usage (3ê°œì›” í›„)**

â€¢ **ì¼ ì˜ˆì¸¡ ê±´ìˆ˜**: 1,350ê±´/ì¼ (ì›” 40,000ê±´)
â€¢ **API ì‘ë‹µ ì†ë„**: í‰ê·  42ms (p95: 78ms)
â€¢ **ì˜ˆì¸¡ ì •í™•ë„**: MAE 6.8ë¶„ ìœ ì§€ (ëª©í‘œ: 8ë¶„ ì´í•˜)
â€¢ **ì‹œìŠ¤í…œ ê°€ìš©ì„±**: 99.7% (ëª©í‘œ: 99.5% ì´ìƒ)

**Monitoring Dashboard**

Grafanaë¡œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§:
â€¢ ì‹œê°„ëŒ€ë³„ ì˜ˆì¸¡ ì •í™•ë„
â€¢ MAE/RMSE íŠ¸ë Œë“œ (ì¼/ì£¼/ì›”)
â€¢ Feature drift íƒì§€ (ë¶„í¬ ë³€í™” ì•Œë¦¼)
â€¢ ë¶ˆë§Œ ì ‘ìˆ˜ ê±´ìˆ˜ ì—°ë™

---

## ğŸ”— Links

[Model API Docs](ë§í¬) | [Monitoring Dashboard](ë§í¬) | [A/B Test Results](ë§í¬) | [GitHub Repo](ë§í¬)
