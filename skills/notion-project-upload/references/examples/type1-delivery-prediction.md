# Delivery Time Prediction System

> ğŸ’¡ **"XGBoost ì˜ˆì¸¡ ëª¨ë¸ë¡œ ë°°ë‹¬ ì‹œê°„ ì˜¤ì°¨ -56%, CSAT +15 points ë‹¬ì„±"**
>
> ë¶€ì •í™•í•œ ë°°ë‹¬ ì‹œê°„ ì•ˆë‚´ ë¬¸ì œë¥¼ ML ëª¨ë¸ë¡œ í•´ê²°í•˜ì—¬ ê³ ê° ë§Œì¡±ë„ ê°œì„  ë° ì¬ì£¼ë¬¸ë¥  ì¦ê°€

---

## ğŸ¯ Performance Overview

**30ì´ˆ ìŠ¤ìº”ìš© - í•µì‹¬ ì„±ê³¼**

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Prediction MAE | 23ë¶„ | 10ë¶„ | **-56%** |
| CSAT Score | 3.8/5 | 4.5/5 | **+15 points** |
| ì¬ì£¼ë¬¸ë¥  (30ì¼) | 61% | 68% | **+7%p** |
| ì—°ê°„ ë§¤ì¶œ | - | +$600K | **ROI 1,422%** |

**Impact Summary**: XGBoost ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶• â†’ MAE 23ë¶„â†’10ë¶„ (-56%) â†’ CSAT +15 points, ì¬ì£¼ë¬¸ë¥  +7%p â†’ ì—° ë§¤ì¶œ +$600K

---

## ğŸ“Š Solution Process

### 1ï¸âƒ£ Problem Discovery

**Business Pain Point**

â€¢ ë¶€ì •í™•í•œ ë°°ë‹¬ ì‹œê°„ ì•ˆë‚´ë¡œ ê³ ê° ë¶ˆë§Œ ì¦ê°€
â€¢ CSAT 3.8/5 (ì—…ê³„ í‰ê·  4.2/5 ëŒ€ë¹„ -0.4ì )
â€¢ ë°°ë‹¬ ì§€ì—°ìœ¼ë¡œ ì¸í•œ ì—°ê°„ í™˜ë¶ˆ ìš”ì²­ $120K
â€¢ ì¬ì£¼ë¬¸ë¥  ì €í•˜ (-12%)ë¡œ ë§¤ì¶œ ì†ì‹¤

**Root Cause Analysis**

â€¢ ê¸°ì¡´ ì‹œìŠ¤í…œ: ë‹¨ìˆœ ê±°ë¦¬ ê¸°ë°˜ í‰ê·  ê³„ì‚°
â€¢ êµí†µ í˜¼ì¡ë„, ë‚ ì”¨, í”¼í¬ ì‹œê°„ëŒ€ ë¯¸ë°˜ì˜
â€¢ ëŸ¬ì‹œì•„ì›Œ(12-14ì‹œ, 18-20ì‹œ) ì˜¤ì°¨ ìµœëŒ€ +45ë¶„
â€¢ ìŒì‹ì ë³„ ì¡°ë¦¬ ì‹œê°„ í¸ì°¨ ë¯¸ê³ ë ¤ (í•œì‹ 18ë¶„ vs ì¤‘ì‹ 24ë¶„)

---

### 2ï¸âƒ£ Solution Design

**Approach & Strategy**

ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ë°°ë‹¬ ì‹œê°„ ì˜ˆì¸¡ ëª¨ë¸ë¡œ ì •í™•ë„ í–¥ìƒ

**Solution Options Considered**

| Option | Pros | Cons | Decision |
|--------|------|------|----------|
| Linear Regression | ê°„ë‹¨, ë¹ ë¦„ | MAE 21ë¶„, ì •í™•ë„ ë‚®ìŒ | âŒ ì„ íƒ ì•ˆ í•¨ |
| Random Forest | MAE 12ë¶„ | ì¶”ë¡  ì†ë„ ëŠë¦¼ (45ms) | âŒ ì„ íƒ ì•ˆ í•¨ |
| XGBoost | MAE 10ë¶„, ë¹ ë¦„ (12ms) | í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš” | âœ… **ì„ íƒ** |

**Feature Engineering** (14 features)

â€¢ Geographic (5): ì§ì„  ê±°ë¦¬, ê²½ë¡œ ê±°ë¦¬, êµí†µ í˜¼ì¡ë„, ì‹ í˜¸ë“± ê°œìˆ˜, ì§€ì—­êµ¬
â€¢ Temporal (4): ì‹œê°„ëŒ€, ìš”ì¼, ê³µíœ´ì¼, ì£¼ë¬¸ëŸ‰
â€¢ Contextual (5): ë‚ ì”¨, ì˜¨ë„, ê°•ìˆ˜ëŸ‰, ìŒì‹ì  ì¡°ë¦¬ ì‹œê°„, ë°°ë‹¬ ê¸°ì‚¬ ê²½ë ¥

**A/B Test Design**

â€¢ **Control**: ê¸°ì¡´ í‰ê·  ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ (N=15,000)
â€¢ **Treatment**: XGBoost ì˜ˆì¸¡ ëª¨ë¸ (N=15,000)
â€¢ **Duration**: 2ì£¼
â€¢ **Primary Metric**: MAE (Mean Absolute Error)
â€¢ **Secondary Metrics**: CSAT, ì¬ì£¼ë¬¸ë¥ , í™˜ë¶ˆ ìš”ì²­ë¥ 
â€¢ **Statistical Power**: 95% confidence, Î±=0.05

---

### 3ï¸âƒ£ Implementation

**Tech Stack**

â€¢ **Model**: Python 3.11, XGBoost 2.0, scikit-learn 1.3
â€¢ **Serving**: FastAPI 0.104, Redis 7.0 (model caching)
â€¢ **Infrastructure**: AWS Lambda (prediction API), S3 (model storage)
â€¢ **Monitoring**: Datadog APM, Mixpanel (CSAT tracking)

**System Architecture**

```mermaid
graph LR
    A[Order Request] --> B[FastAPI Gateway]
    B --> C[Redis Cache]
    C -->|Cache Hit| D[Cached Prediction]
    C -->|Cache Miss| E[XGBoost Model]
    E --> F[Feature Engineering]
    F --> G[Prediction 12ms]
    G --> H[Update Cache]
    G --> I[Response]
```

**Core Implementation**

```python
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit

# Feature engineering with 14 features
def create_features(df):
    df['adjusted_distance'] = df['distance'] * df['traffic_factor']
    df['peak_hour'] = df['hour'].isin([12, 13, 18, 19, 20])
    df['weather_penalty'] = df['precipitation'] * 0.3
    # ... 11 more features
    return df

# Time-series cross-validation (6 folds)
tscv = TimeSeriesSplit(n_splits=6)
for train_idx, val_idx in tscv.split(features):
    model = xgb.XGBRegressor(
        max_depth=6,
        learning_rate=0.1,
        n_estimators=200,
        objective='reg:squarederror'
    )
    model.fit(X_train, y_train)
    
# FastAPI prediction endpoint
@app.post("/predict")
async def predict(request: PredictRequest):
    features = extract_features(request)
    prediction = model.predict(features)
    return {"estimated_time": prediction[0]}
```

**Implementation Highlights**

â€¢ Real-time prediction API: í‰ê·  ì‘ë‹µ ì‹œê°„ 12ms
â€¢ Redis caching: ë™ì¼ ê²½ë¡œ ìš”ì²­ ì¦‰ì‹œ ì‘ë‹µ
â€¢ Fallback mechanism: ëª¨ë¸ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìë™ ì „í™˜
â€¢ Feature importance: êµí†µ í˜¼ì¡ë„ (32%), ê±°ë¦¬ (28%), ì‹œê°„ëŒ€ (21%)

---

### 4ï¸âƒ£ Validation & Testing

**Offline Evaluation**

| Metric | Baseline | XGBoost | Improvement |
|--------|----------|---------|-------------|
| MAE | 23ë¶„ | 10ë¶„ | **-56%** |
| P95 | 37ë¶„ | 18ë¶„ | **-51%** |
| RMSE | 28ë¶„ | 13ë¶„ | **-54%** |

**A/B Test Results** (2ì£¼, N=30,000)

â€¢ Treatment MAE: **10.4ë¶„** vs Control: 22.8ë¶„ (-54%, **p<0.001**)
â€¢ CSAT: **4.5/5** vs 3.8/5 (+15 points, **p<0.001**)
â€¢ ì¬ì£¼ë¬¸ë¥ : **68%** vs 61% (+7%p, **p<0.001**)
â€¢ í™˜ë¶ˆ ìš”ì²­: **-62%** reduction

**Error Analysis**

â€¢ ë‚ ì”¨ ê·¹ë‹¨ê°’ (í­ìš°/í­ì„¤): ì˜¤ì°¨ +8ë¶„ â†’ ë³„ë„ ë‚ ì”¨ ëª¨ë¸ í•„ìš”
â€¢ ì‹ ê·œ ì§€ì—­: ë°ì´í„° ë¶€ì¡± â†’ Fallback to baseline
â€¢ í”¼í¬ ì‹œê°„ (19-20ì‹œ): ì˜¤ì°¨ Â±6ë¶„ â†’ ê°€ì¥ ì •í™•í•œ êµ¬ê°„

---

### 5ï¸âƒ£ Deployment & Rollout

**Rollout Strategy**

```mermaid
gantt
    title Deployment Timeline
    dateFormat  YYYY-MM-DD
    
    section Testing
    A/B Test           :done, 2024-01-01, 14d
    
    section Rollout
    Canary 5%         :done, 2024-01-15, 2d
    Expand to 25%     :done, 2024-01-17, 3d
    Expand to 50%     :done, 2024-01-20, 3d
    Full Rollout 100% :done, 2024-01-23, 1d
```

â€¢ **Phase 1 (Canary 5%)**: 2ì¼, MAE ëª¨ë‹ˆí„°ë§, ì—ëŸ¬ìœ¨ <1% í™•ì¸
â€¢ **Phase 2 (25%)**: 3ì¼, CSAT ê°œì„  í™•ì¸ (+12 points)
â€¢ **Phase 3 (50%)**: 3ì¼, ì¬ì£¼ë¬¸ë¥  ì¦ê°€ í™•ì¸ (+5%p)
â€¢ **Phase 4 (100%)**: 1ì¼, ì „ì²´ ë¡¤ì•„ì›ƒ

**Monitoring & Alerting**

â€¢ **Datadog**: Prediction latency, error rate, model drift
â€¢ **Mixpanel**: CSAT tracking, ì¬ì£¼ë¬¸ë¥ , í™˜ë¶ˆ ìš”ì²­ë¥ 
â€¢ **Alert**: MAE >15ë¶„ ì‹œ Slack #eng-ml ì•Œë¦¼
â€¢ **Dashboard**: Real-time prediction accuracy, cache hit rate (87%)

**Production Infrastructure**

â€¢ AWS Lambda: Auto-scaling (100 â†’ 500 concurrent)
â€¢ S3: Model versioning, rollback ì§€ì›
â€¢ Redis: Model caching, TTL 90ì´ˆ
â€¢ CI/CD: GitHub Actions â†’ Lambda deployment

---

### 6ï¸âƒ£ Impact Measurement

**Business Impact** (ìš´ì˜ 3ê°œì›” í›„)

| í•­ëª© | Before | After | Impact |
|------|--------|-------|--------|
| CSAT | 3.8/5 | 4.5/5 | **+15 points** |
| ì¬ì£¼ë¬¸ìœ¨ | 61% | 68% | **+7%p** |
| ì—°ê°„ ë§¤ì¶œ | - | +$600K | **ì‹ ê·œ** |
| í™˜ë¶ˆ ë¹„ìš© | $120K/ë…„ | $35K/ë…„ | **-$85K** |

**ROI Calculation**

```
ê°œë°œ ë¹„ìš©: $45K (3ëª… Ã— 2ê°œì›”)
ì—°ê°„ ìˆ˜ìµ: $600K (ë§¤ì¶œ) + $85K (í™˜ë¶ˆ ì ˆê°) = $685K
ROI: ($685K - $45K) / $45K = 1,422%
ì—°í™˜ì‚° ROI: 340%
```

**Operational Efficiency**

â€¢ ê³ ê° ë¶ˆë§Œ ì „í™”: ì›” 1,200ê±´ â†’ 480ê±´ (**-60%**)
â€¢ CS ëŒ€ì‘ ì‹œê°„: í‰ê·  8ë¶„ â†’ 3ë¶„ (**-62%**)
â€¢ ë°°ë‹¬ì› ë§Œì¡±ë„: +12 points (ì •í™•í•œ ì‹œê°„ â†’ íš¨ìœ¨ì  ê²½ë¡œ)
â€¢ ë°°ë‹¬ ê¸°ì‚¬ ëŒ€ê¸° ì‹œê°„: 4.2ë¶„ â†’ 2.1ë¶„ (**-50%**)

**Long-term Impact**

â€¢ 3ê°œì›”: CSAT 4.5 ìœ ì§€, ì¬ì£¼ë¬¸ë¥  68% ì•ˆì •í™”
â€¢ 6ê°œì›”: ì¶”ê°€ ê°œì„  ê¸°íšŒ ë°œê²¬ (ë‚ ì”¨ ëª¨ë¸, ìŒì‹ì ë³„ ëª¨ë¸)
â€¢ 12ê°œì›” ê³„íš: ë‹¤ë¥¸ ë„ì‹œ í™•ì¥, ë©€í‹°ëª¨ë‹¬ ì˜ˆì¸¡ (ìì „ê±°/ë„ë³´)

---

## ğŸ’¡ Key Takeaways

**"Feature Engineering > Model Selection"**

êµí†µ í˜¼ì¡ë„ ë°˜ì˜í•œ "adjusted_distance" í”¼ì²˜ê°€ MAE 2.3ë¶„ ê°œì„ . Raw dataë³´ë‹¤ ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ í”¼ì²˜ê°€ ì¤‘ìš”.

**ì•„ì‰¬ìš´ ì  & ê°œì„  ë°©í–¥**

â€¢ **í•œê³„ì **: ë‚ ì”¨ ê·¹ë‹¨ê°’ (í­ìš°/í­ì„¤) ì‹œ ì˜¤ì°¨ +8ë¶„, ì‹ ê·œ ì§€ì—­ ë°ì´í„° ë¶€ì¡±
â€¢ **Next Step**: ë‚ ì”¨ë³„ ì „ìš© ëª¨ë¸, ìŒì‹ì ë³„ ì¡°ë¦¬ ì‹œê°„ ì˜ˆì¸¡ ëª¨ë¸ ì¶”ê°€

---

## ğŸ¤ Collaboration & Impact

**Teams Involved**

â€¢ **Data Science** (ë‚˜): ëª¨ë¸ ê°œë°œ, Feature Engineering, A/B í…ŒìŠ¤íŠ¸ ì„¤ê³„
â€¢ **Engineering**: FastAPI êµ¬ì¶•, Redis ìºì‹±, Lambda ë°°í¬ íŒŒì´í”„ë¼ì¸
â€¢ **Product**: A/B í…ŒìŠ¤íŠ¸ ê¸°íš, CSAT ì¶”ì , ë¡¤ì•„ì›ƒ ì „ëµ
â€¢ **Operations**: ë°°ë‹¬ì› í”¼ë“œë°± ìˆ˜ì§‘, í”„ë¡œì„¸ìŠ¤ ê°œì„  ì œì•ˆ

**My Contribution**

â€¢ XGBoost ëª¨ë¸ ê°œë°œ ë° 14ê°œ feature engineering
â€¢ Time-series cross-validation ì„¤ê³„ (6-fold)
â€¢ A/B í…ŒìŠ¤íŠ¸ í†µê³„ ë¶„ì„ ë° ìœ ì˜ì„± ê²€ì¦
â€¢ FastAPI ì˜ˆì¸¡ ì„œë¹„ìŠ¤ êµ¬ì¶• ë° Redis ìºì‹±
â€¢ Datadog ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ì„¤ì •

**Communication & Feedback**

â€¢ ë°°ë‹¬ íŒ€ ì£¼ 1íšŒ ë¯¸íŒ…: "ì‹ í˜¸ë“± ê°œìˆ˜" í”¼ì²˜ ì¶”ê°€ ì œì•ˆ â†’ MAE 1.1ë¶„ ê°œì„ 
â€¢ CS íŒ€: ì˜ˆì¸¡ ì‹ ë¢° êµ¬ê°„ í‘œì‹œ ìš”ì²­ â†’ "25-30ë¶„ ë„ì°© ì˜ˆì •" UI ê°œì„ 
â€¢ ê²½ì˜ì§„: ROI 1,422% ë³´ê³  â†’ ë‹¤ë¥¸ ë„ì‹œ í™•ì¥ ìŠ¹ì¸

---

## ğŸ”— Links

[GitHub](https://github.com/example/delivery-prediction) | [Dashboard](https://datadog.com/dashboard/delivery) | [A/B Test Report](https://docs.google.com/ab-test-report)

---

## ğŸ¤ Collaboration & Impact

**Cross-functional Collaboration**

**ë°°ë‹¬ ìš´ì˜íŒ€** (PM: ê¹€í˜„ìˆ˜)
â€¢ ì£¼ 1íšŒ ì„±ê³¼ ë¦¬ë·° ë¯¸íŒ… - ì˜ˆì¸¡ ì •í™•ë„ ëª¨ë‹ˆí„°ë§ ë° ì´ìŠˆ íŠ¸ë˜í‚¹
â€¢ ë°°ë‹¬ ê¸°ì‚¬ 40ëª… ì¸í„°ë·°ë¡œ ì‹¤ì œ ë°°ë‹¬ ì‹œ ë³€ìˆ˜ íŒŒì•… (ì‹ í˜¸ë“±, ì£¼ì°¨, ì—˜ë¦¬ë² ì´í„° ë“±)
â€¢ í”¼ë“œë°±: "êµí†µ í˜¼ì¡ë„" í”¼ì²˜ ì¶”ê°€ ì œì•ˆ â†’ MAE 1.8ë¶„ ê°œì„ 

**ìŒì‹ì  íŒŒíŠ¸ë„ˆíŒ€** (ë§¤ë‹ˆì €: ì´ì§€ì—°)
â€¢ 200ê°œ íŒŒíŠ¸ë„ˆ ìŒì‹ì  ì¡°ë¦¬ ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ í˜‘ì—…
â€¢ ìŒì‹ ì¢…ë¥˜ë³„ í‰ê·  ì¡°ë¦¬ ì‹œê°„ ë²¤ì¹˜ë§ˆí¬ êµ¬ì¶•
â€¢ ê²°ê³¼: ì¡°ë¦¬ ì‹œê°„ ì˜ˆì¸¡ ì •í™•ë„ 68% â†’ 84% í–¥ìƒ

**CSíŒ€** (íŒ€ì¥: ë°•ë¯¼ì§€)
â€¢ ë¶ˆë§Œ ì ‘ìˆ˜ ë‚´ì—­ ë¶„ì„ìœ¼ë¡œ ì£¼ìš” pain point íŒŒì•…
â€¢ "ì˜ˆì¸¡ ì‹ ë¢° êµ¬ê°„" í‘œì‹œ ì œì•ˆ ìˆ˜ìš© â†’ ëª¨í˜¸í•¨ ê°ì†Œ
â€¢ ê²°ê³¼: ë°°ë‹¬ ì‹œê°„ ê´€ë ¨ ë¶ˆë§Œ 60% ê°ì†Œ

**Stakeholder Impact**

â€¢ **ê³ ê°**: ì •í™•í•œ ë„ì°© ì‹œê°„ ì•ˆë‚´ë¡œ ëŒ€ê¸° ìŠ¤íŠ¸ë ˆìŠ¤ ê°ì†Œ, CSAT +15ì 
â€¢ **ë°°ë‹¬ ê¸°ì‚¬**: ìµœì  ê²½ë¡œ ì¶”ì²œìœ¼ë¡œ ì‹œê°„ë‹¹ ë°°ë‹¬ ê±´ìˆ˜ +19%, ìˆ˜ì… ì¦ê°€
â€¢ **ìŒì‹ì **: ì£¼ë¬¸ ì¤€ë¹„ íƒ€ì´ë° ìµœì í™”ë¡œ ìŒì‹ í’ˆì§ˆ ìœ ì§€
â€¢ **íšŒì‚¬**: ì¬ì£¼ë¬¸ë¥  ìƒìŠ¹ìœ¼ë¡œ ì—° ë§¤ì¶œ +$600K

---

## ğŸ”§ Technical Approach

### Model Architecture

```mermaid
graph LR
    A[Raw Data] --> B[Feature Engineering]
    B --> C[XGBoost Model]
    C --> D[Prediction + CI]
    D --> E[API Response]
    
    B --> F[19 Features]
    F --> F1[ì£¼ë¬¸ ì •ë³´ 5ê°œ]
    F --> F2[ê±°ë¦¬/ê²½ë¡œ 4ê°œ]
    F --> F3[ìŒì‹ì  3ê°œ]
    F --> F4[ë°°ë‹¬ ê¸°ì‚¬ 4ê°œ]
    F --> F5[í™˜ê²½ 3ê°œ]
    
    C --> G[Hyperparameters]
    G --> G1[max_depth: 7]
    G --> G2[learning_rate: 0.05]
    G --> G3[n_estimators: 300]
```

### Key Implementation

**Feature Engineering - Traffic Congestion Score**

```python
import pandas as pd
import numpy as np
from datetime import datetime

def calculate_traffic_congestion(row):
    """
    êµí†µ í˜¼ì¡ë„ ê³„ì‚° - ì‹œê°„ëŒ€, ìš”ì¼, ë‚ ì”¨ ë°˜ì˜
    ë°˜í™˜ê°’: 0.5 (í•œì‚°) ~ 2.5 (ë§¤ìš° í˜¼ì¡)
    """
    base_congestion = 1.0
    
    # ì‹œê°„ëŒ€ ê°€ì¤‘ì¹˜
    hour = row['order_hour']
    if 12 <= hour <= 14 or 18 <= hour <= 20:  # ëŸ¬ì‹œì•„ì›Œ
        base_congestion *= 1.8
    elif 22 <= hour or hour <= 6:  # ì‹¬ì•¼
        base_congestion *= 0.6
    
    # ìš”ì¼ ê°€ì¤‘ì¹˜
    if row['is_weekend']:
        base_congestion *= 1.2
    
    # ë‚ ì”¨ ê°€ì¤‘ì¹˜
    if row['weather'] == 'rain':
        base_congestion *= 1.4
    elif row['weather'] == 'snow':
        base_congestion *= 1.8
    
    return np.clip(base_congestion, 0.5, 2.5)

# ì ìš©
df['traffic_congestion'] = df.apply(calculate_traffic_congestion, axis=1)
df['adjusted_distance'] = df['route_distance_km'] * df['traffic_congestion']

# Impact: MAE 9.1ë¶„ â†’ 6.8ë¶„ (-25%)
```

**XGBoost Model Training**

```python
from xgboost import XGBRegressor
from sklearn.model_selection import cross_val_score
import numpy as np

# ëª¨ë¸ ì •ì˜
model = XGBRegressor(
    max_depth=7,
    learning_rate=0.05,
    n_estimators=300,
    subsample=0.8,
    colsample_bytree=0.8,
    objective='reg:squarederror',
    random_state=42
)

# 5-Fold Cross-Validation
cv_scores = cross_val_score(
    model, X_train, y_train,
    cv=5,
    scoring='neg_mean_absolute_error'
)

print(f"CV MAE: {-cv_scores.mean():.2f} Â± {cv_scores.std():.2f}ë¶„")
# Output: CV MAE: 6.95 Â± 0.31ë¶„

# ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ
model.fit(X_train, y_train)

# í…ŒìŠ¤íŠ¸ ì„±ëŠ¥
y_pred = model.predict(X_test)
test_mae = np.mean(np.abs(y_test - y_pred))
print(f"Test MAE: {test_mae:.2f}ë¶„")
# Output: Test MAE: 6.82ë¶„
```

---

## ğŸš€ Deployment & Usage

**Production System**

```mermaid
graph TB
    A[ì£¼ë¬¸ ë°œìƒ] --> B[Feature ìˆ˜ì§‘]
    B --> C{Cache Hit?}
    C -->|Yes| D[ìºì‹œëœ ì˜ˆì¸¡ê°’]
    C -->|No| E[Model API í˜¸ì¶œ]
    E --> F[ì˜ˆì¸¡ + ì‹ ë¢°êµ¬ê°„]
    F --> G[Cache ì €ì¥]
    D --> H[ê³ ê° ì•± í‘œì‹œ]
    G --> H
    
    I[Daily Retrain] --> J[ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§]
    J --> K{ì„±ëŠ¥ ì €í•˜?}
    K -->|Yes| L[Alert + ì¬í•™ìŠµ]
    K -->|No| I
```

**Deployment Timeline**

| Phase | Duration | Activity | Result |
|-------|----------|----------|--------|
| ê°œë°œ | 6ì£¼ | ë°ì´í„° ìˆ˜ì§‘, ëª¨ë¸ í•™ìŠµ, API ê°œë°œ | MAE 6.8ë¶„ ë‹¬ì„± |
| A/B í…ŒìŠ¤íŠ¸ | 2ì£¼ | 10% íŠ¸ë˜í”½ í…ŒìŠ¤íŠ¸ | ì´ìŠˆ ì—†ìŒ í™•ì¸ |
| ì ì§„ì  ë°°í¬ | 2ì£¼ | 10% â†’ 50% â†’ 100% | ì•ˆì •ì„± í™•ì¸ |
| ëª¨ë‹ˆí„°ë§ | ì§„í–‰ì¤‘ | ì¼ì¼ ì„±ëŠ¥ ì²´í¬, ì£¼ê°„ ë¦¬í¬íŠ¸ | CSAT ì§€ì† ìƒìŠ¹ |

**Current Usage (3ê°œì›” í›„)**

â€¢ **ì¼ ì˜ˆì¸¡ ê±´ìˆ˜**: 1,350ê±´/ì¼ (ì›” 40,000ê±´)
â€¢ **API ì‘ë‹µ ì†ë„**: í‰ê·  42ms (p95: 78ms)
â€¢ **ì˜ˆì¸¡ ì •í™•ë„**: MAE 6.8ë¶„ ìœ ì§€ (ëª©í‘œ: 8ë¶„ ì´í•˜)
â€¢ **ì‹œìŠ¤í…œ ê°€ìš©ì„±**: 99.7% (ëª©í‘œ: 99.5% ì´ìƒ)

**Monitoring Dashboard**

Grafanaë¡œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§:
â€¢ ì‹œê°„ëŒ€ë³„ ì˜ˆì¸¡ ì •í™•ë„
â€¢ MAE/RMSE íŠ¸ë Œë“œ (ì¼/ì£¼/ì›”)
â€¢ Feature drift íƒì§€ (ë¶„í¬ ë³€í™” ì•Œë¦¼)
â€¢ ë¶ˆë§Œ ì ‘ìˆ˜ ê±´ìˆ˜ ì—°ë™

---

## ğŸ”— Links

[Model API Docs](ë§í¬) | [Monitoring Dashboard](ë§í¬) | [A/B Test Results](ë§í¬) | [GitHub Repo](ë§í¬)
